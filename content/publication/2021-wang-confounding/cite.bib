@article{wang2021confounding,
title = {Learning Gaussian graphical models with latent confounders},
journal = {Journal of Multivariate Analysis},
pages = {105213},
year = {2023},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2023.105213},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X23000593},
author = {Ke Wang and Alexander Franks and Sang-Yun Oh},
keywords = {Gaussian graphical models, High-dimensional data, Latent variables, Principal component analysis},
abstract = {Gaussian Graphical models (GGM) are widely used to estimate network structure in domains ranging from biology to finance. In practice, data is often corrupted by latent confounders which biases inference of the underlying true graphical structure. In this paper, we compare and contrast two strategies for inference in graphical models with latent confounders: Gaussian graphical models with latent variables (LVGGM) and PCA-based removal of confounding (PCA+GGM). While these two approaches have similar goals, they are motivated by different assumptions about confounding. In this paper, we explore the connection between these two approaches and propose a new method, which combines the strengths of these two approaches. We prove the consistency and convergence rate for the PCA-based method and use these results to provide guidance about when to use each method. We demonstrate the effectiveness of our methodology using both simulations and two real-world applications.}
}